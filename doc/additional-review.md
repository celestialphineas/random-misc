## 文献综述

这一章是对本研究在计算方面的补充文献综述，将详细介绍研究中使用到算法和工具。

### 聚类分析

本节将介绍研究中使用到的聚类分析相关算法，行文并非依聚类模型算法组织，而是按照聚类分析于关联领域的各个方面进行介绍。

聚类分析是无监督学习领域的分支，是将对象按某一相似度（距离）去分组（即聚类）的任务 {{The Elements of Statistical Learning}}。每簇中的元素都被分到一个类别变量的值。聚类算法可粗略分为以下几类：基于联通性（分层聚类）、基于中心、基于分布等。对于不同问题也有不同的对距离函数的定义。而每个对象分到的类别变量的值也可以是一个列表，即模糊聚类。

#### 聚类模型

本研究中使用到的聚类模型包括基于中心的方法（如*k*-均值）和分层聚类方法。下简要介绍后者。

分层聚类的实现既可以自底向上，也可以自上而下。自底向上的分层聚类（聚合法，agglomerative）从点开始形成簇；自上而下的分层聚类（分割法，divisible）从一个簇开始，通过分割形成多个簇。

分层聚类模型的输出是一个树状图（dendrogram），通常以二叉树的形式出现，这颗树保留了相邻和距离信息。通过切割树状图，即可得到若干个簇。

#### 距离函数

除了*p*-范数，本研究中也经常使用到Jaccard不相似度（Jaccard dissimilarity）。$x=\left(x_1,x_2,\dots,x_n\right),y=\left(y_1,y_2,\dots,y_n\right)$ 是两个$n$维向量，其中$x_i,y_i\in\left\{0,1\right\}$，$x$和$y$的Jaccard不相似度定义为

$$J(x,y)=\sum_{i=1}^n x\oplus y$$

$\oplus$是异或运算符。

Jaccard不相似度经常用来度量二进制矩阵的距离，也可以描述集合之间的差异。两个集合$X$和$Y$的Jaccard**相似度**定义为

$$J(X,Y)=\frac{\left|X\cap Y\right|}{\left|X\cup Y\right|}$$

### 确定簇数

确定数据集的簇数并不容易。基于中心的聚类模型需要将簇数作为输入；对分层聚类而言，我们也需要簇数去分割树状图。研究中我们使用到了弯折点法（elbow method），是聚类分析中确定簇数的常见方法。

拐点法计算每个簇内部的方差，再产生一组方差总和关于簇数的点，在这组点中寻找弯折点，其对应的簇数可作为簇数确定时的一个参考。弯折点大多数时候选取使方差总和下降较快处，但并非常常用严格的数学方法找到弯折点。对分层聚类，这个过程可以通过迭代地去分割树状图来快速计算。本文中实现了分层聚类弯折点法的可视化算法。

下图为使用弯折点法的一例，数据来自原始测序数据向协同组装（co-assembly）得到的连续片段（contigs）对齐（align）的结果。每个细胞的测序数据都可以对应于一个向量，其每个维度对应于在一个连续片段上的测序深度。我们可将这一向量作为特征对细胞聚类。

{{Elbow image}}

{{Elbow 距离矩阵，数据被分为25个簇，该数值根据弯折点图线确定}}

在分层聚类中，确定簇数也可以基于指定距离的方式，例如通过指定簇内最大距离、最小距离或是平均距离，来分割树状图。SciPy包含了这些功能。

### 降维与可视化

聚类分析结果的可视化方法包括绘制距离矩阵，以及利用降维方法获得的散点图。为了实现高维数据的可视化，可将高维数据降维到一维、二维或三维，再用空间位置编码降维后的点绘制出来，即可实现对高维数据的可视化。降维算法也经常用于聚类算法的数据预处理。{{Ding 2004}}

下文介绍我们使用到的降维算法，以及我们所关心的主要性质。

#### 主成分分析

主成分分析（principal component analysis, PCA）对数据做正交变换，使得变换后的点集在第一个主成分上方差最大，后续成分的方差在满足正交性的情况下依次为最大值。数学上，PCA可以通过协方差矩阵的特征值分解或数据矩阵的奇异值分解来实现。

PCA约束具有较好的性质，其与*k*-均值算法聚类指标（cluster indicator）的连续情形的等价性可以被证明 {{Ding 2004}}。PCA用于在聚类前处理高维数据，也可以作为在正交投影上保留距离的一种可视化方法。

#### 多维缩放

多维缩放（multidimensional scaling, MDS）以距离矩阵作为输入，以特定空间中的点作为输出。多维缩放可以作为一种非线性的降维方法。

$\left[d_{ij}\right]$为距离矩阵，其中$d_{ij}$是第$i$个点和第$j$个点的距离，$x_i (i=1,2,\dots,N)$表示$N$个输出点。$\delta_{ij}$定义为$x_i$和$x_j$间的不相似度。度量多维缩放（metric multidimensional scaling, mMDS）将如下损失函数最小化 {{Multidimensional scaling}}：

$$Strain\left(x_1,x_2,\dots,x_N\right)=\frac{\sum_{i,j}\left(d_{ij}-\delta_{ij}\right)^2}{\sum_{ij}d_{ij}^2}$$

MDS方法在一定程度上保留距离信息，但在镶嵌到另一空间的过程中丢失了一些信息。

#### *t*-SNE

*t*-SNE（*t*-distributed stochastic neighbor embedding，*t*-分布型随机近邻镶嵌）是一个基于概率的降维方法。*t*-SNE定义了概率$p_{ij}$，与高维对象间的相似度成比例，算法会学习到一个映射来最小化新旧空间中的点的距离分布的差别。

*t*-SNE并不能保距离和保密度，但是近邻关系可以被很好地保留 {{Schubert 2017}}。这个性质使得*t*-SNE不能被用作聚类预处理的方法。*t*-SNE映射是逐点的，在有新数据加进来的时候，新点无法被很快地映射到低维空间中。

### 工具方法

#### Mash和Sourmash

Mash使用MinHash算法，无需对齐（align）即可比较序列相似度 {{Ondov 2015}}，Sourmash是对该算法的另一实现 {{Brown 2016}}。两个工具都可以快速地从输入序列中提取出签名（signature）作为原序列的特征，通过计算两个签名之间的不相似度，即可估计两个原序列的差异程度，且该估计值与平均核苷酸一致性（average nucleotide identity, ANI）有着较好的相关性 {{Ondov 2015}}。

MinHash算法可以基于概率快速估计两个集合的Jaccard相似度。与四核苷酸频率（tetranucleotide frequency, TNF）相比，基于MinHash的算法可以将TNF的4-mer比较推广至更大的*k*值。对于较大的$k$值，直接比较两个*k*-mer统计（每个维度的含义是一个*k*-mer种类，值是这个*k*-mer在序列中出现的次数）是不可行的，因为*k*-mer的种类对$k$是成指数增长的。随着$k$的变大，*k*-mer统计向量也会变得越来越稀疏。在这种情况下，用*k*-mer集合的不相似度就与使用*k*-mer统计向量的不相似度几乎等价，因为后者对固定大小的输入，在$k$足够大时，其大多数元素都会是0和1。在这一意义下，基于MinHash的算法可以视作基于*k*-mer统计的算法在$k$较大情况下推广的近似。

Sourmash默认使用$k=31$。相比于TNF中的$k=4$，基于MinHash的算法可以更好地保留输入序列的模式信息。

#### Kaiju

Kaiju是一个对序列做分类学（taxonomic）分类（classify）工具，可以快速地预测读与contig的分类单元（taxon）。Kaiju将输入序列转换为蛋白质序列后再查找，由于蛋白质序列的保守性（conservativeness）和简并性（degeneracy）蛋白质序列查找相比于DNA序列查找更加健壮。

Kaiju利用Burrows–Wheeler变换（BWT）压缩数据库与快速查找序列。BWT算法也常用于序列对齐工具，例如Bowtie和BWA，该算法有着较好的查找效率和较小的内存占用。{{Menzel 2015}}

本文利用Kaiju作为单细胞测序数据的分类工具。

#### 其他序列分类工具

我们也使用到了rMLST的线上API对基因组组装草图（draft genome assembly）做分类学分类。rMLST利用核糖体多位点序列（ribosomal multilocus sequence）分类，能给出分类单元预测和对应分数。{{Jolley 2012}}

CAT（contig annotation tool）是对长DNA序列的分类学分类工具，它可以结合多个分类工具和方法的预测结果给出一个更加准确的预测。{{Cambuy 2016}}

#### DAS Tool

DAS Tool是结合多个装箱（bin）方法结果的一套方法流程。它用去重（dereplication）、聚合（aggregation）和打分（scoring）的策略去优化装箱结果，可以用于宏基因组学研究。DAS Tol可以提升组装质量，降低装箱冗余。{{Sieber 2018}} 

### 质量评估

质量评估与控制在数据处理流程中最晚，但也相当重要。基因组箱（genome bin）的质量指标包括完整性和污染度（contamination，与期望基因组大小相比的冗余率），contig长度的平均、中值与总和，以及N50统计（覆盖基因组全长50%的最长contig中的最短的长度）等。

CheckM是检验完整性和污染度的前沿工具，其原理基于估算标志基因（marker gene）与推断基因组的谱系（lineage）。CheckM是我们使用到的主要质量评估与控制工具。{{Parks 2015}}

质量评估与控制的结果也需要可视化的技巧来让读者更加容易理解，并迅速地抓住数据的关键特征。