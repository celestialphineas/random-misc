## 文献综述

这一章是对本研究在计算方面的补充文献综述，将详细介绍研究中使用到算法和工具。

### 聚类分析

本节将介绍研究中使用到的聚类分析相关算法，行文并非依聚类模型算法组织，而是按照聚类分析于关联领域的各个方面进行介绍。

聚类分析是无监督学习领域的分支，是将对象按某一相似度（距离）去分组（即聚类）的任务 {{The Elements of Statistical Learning}}。每簇中的元素都被分到一个类别变量的值。聚类算法可粗略分为以下几类：基于联通性（分层聚类）、基于中心、基于分布等。对于不同问题也有不同的对距离函数的定义。而每个对象分到的类别变量的值也可以是一个列表，即模糊聚类。

#### 聚类模型

本研究中使用到的聚类模型包括基于中心的方法（如*k*-均值）和分层聚类方法。下简要介绍后者。

分层聚类的实现既可以自底向上，也可以自上而下。自底向上的分层聚类（聚合法，agglomerative）从点开始形成簇；自上而下的分层聚类（分割法，divisible）从一个簇开始，通过分割形成多个簇。

分层聚类模型的输出是一个树状图（dendrogram），通常以二叉树的形式出现，这颗树保留了相邻和距离信息。通过切割树状图，即可得到若干个簇。

#### 距离函数

除了*p*-范数，本研究中也经常使用到Jaccard不相似度（Jaccard dissimilarity）。$x=\left(x_1,x_2,\dots,x_n\right),y=\left(y_1,y_2,\dots,y_n\right)$ 是两个$n$维向量，其中$x_i,y_i\in\left\{0,1\right\}$，$x$和$y$的Jaccard不相似度定义为

$$J(x,y)=\sum_{i=1}^n x\oplus y$$

$\oplus$是异或运算符。

Jaccard不相似度经常用来度量二进制矩阵的距离，也可以描述集合之间的差异。两个集合$X$和$Y$的Jaccard**相似度**定义为

$$J(X,Y)=\frac{\left|X\cap Y\right|}{\left|X\cup Y\right|}$$

### 确定簇数

确定数据集的簇数并不容易。基于中心的聚类模型需要将簇数作为输入；对分层聚类而言，我们也需要簇数去分割树状图。研究中我们使用到了弯折点法（elbow method），是聚类分析中确定簇数的常见方法。

拐点法计算每个簇内部的方差，再产生一组方差总和关于簇数的点，在这组点中寻找弯折点，其对应的簇数可作为簇数确定时的一个参考。弯折点大多数时候选取使方差总和下降较快处，但并非常常用严格的数学方法找到弯折点。对分层聚类，这个过程可以通过迭代地去分割树状图来快速计算。本文中实现了分层聚类弯折点法的可视化算法。

下图为使用弯折点法的一例，数据来自原始测序数据向协同组装（co-assembly）得到的连续片段（contigs）对齐（align）的结果。每个细胞的测序数据都可以对应于一个向量，其每个维度对应于在一个连续片段上的测序深度。我们可将这一向量作为特征对细胞聚类。

{{Elbow image}}

{{Elbow 距离矩阵，数据被分为25个簇，该数值根据弯折点图线确定}}

在分层聚类中，确定簇数也可以基于指定距离的方式，例如通过指定簇内最大距离、最小距离或是平均距离，来分割树状图。SciPy包含了这些功能。

### 降维与可视化

聚类分析结果的可视化方法包括绘制距离矩阵，以及利用降维方法获得的散点图。为了实现高维数据的可视化，可将高维数据降维到一维、二维或三维，再用空间位置编码降维后的点绘制出来，即可实现对高维数据的可视化。降维算法也经常用于聚类算法的数据预处理。{{Ding 2004}}

下文介绍我们使用到的降维算法，以及我们所关心的主要性质。

#### 主成分分析

主成分分析（principal component analysis, PCA）对数据做正交变换，使得变换后的点集在第一个主成分上方差最大，后续成分的方差在满足正交性的情况下依次为最大值。数学上，PCA可以通过协方差矩阵的特征值分解或数据矩阵的奇异值分解来实现。

PCA约束具有较好的性质，其与*k*-均值算法聚类指标（cluster indicator）的连续情形的等价性可以被证明 {{Ding 2004}}。PCA用于在聚类前处理高维数据，也可以作为在正交投影上保留距离的一种可视化方法。

#### 多维缩放

多维缩放（multidimensional scaling, MDS）以距离矩阵作为输入，以特定空间中的点作为输出。多维缩放可以作为一种非线性的降维方法。

$\left[d_{ij}\right]$为距离矩阵，其中$d_{ij}$是第$i$个点和第$j$个点的距离，$x_i (i=1,2,\dots,N)$表示$N$个输出点。$\delta_{ij}$定义为$x_i$和$x_j$间的不相似度。度量多维缩放（metric multidimensional scaling, mMDS）将如下损失函数最小化 {{Multidimensional scaling}}：

$$Strain\left(x_1,x_2,\dots,x_N\right)=\frac{\sum_{i,j}\left(d_{ij}-\delta_{ij}\right)^2}{\sum_{ij}d_{ij}^2}$$

MDS方法在一定程度上保留距离信息，但在镶嵌到另一空间的过程中丢失了一些信息。

#### *t*-SNE

*t*-SNE（*t*-distributed stochastic neighbor embedding，*t*-分布型随机近邻镶嵌）是一个基于概率的降维方法。*t*-SNE定义了概率$p_{ij}$，与高维对象间的相似度成比例，算法会学习到一个映射来最小化新旧空间中的点的距离分布的差别。

*t*-SNE并不能保距离和保密度，但是近邻关系可以被很好地保留 {{Schubert 2017}}。这个性质使得*t*-SNE不能被用作聚类预处理的方法。*t*-SNE映射是逐点的，在有新数据加进来的时候，新点无法被很快地映射到低维空间中。

### 工具方法

#### Mash和Sourmash

Mash使用MinHash算法，无需对齐（align）即可比较序列相似度 {{Ondov 2015}}，Sourmash是对该算法的另一实现 {{Brown 2016}}。两个工具都可以快速地从输入序列中提取出签名（signature）作为原序列的特征，通过计算两个签名之间的不相似度，即可估计两个原序列的差异程度，且该估计值与平均核苷酸一致性（average nucleotide identity, ANI）有着较好的相关性 {{Ondov 2015}}。

MinHash算法可以基于概率快速估计两个集合的Jaccard相似度。与四核苷酸频率（tetranucleotide frequency, TNF）相比，基于MinHash的算法可以将TNF的4-mer比较推广至更大的*k*值。对于较大的$k$值，直接比较两个*k*-mer统计（每个维度的含义是一个*k*-mer种类，值是这个*k*-mer在序列中出现的次数）是不可行的，因为*k*-mer的种类对$k$是成指数增长的。随着$k$的变大，*k*-mer统计向量也会变得越来越稀疏。在这种情况下，用*k*-mer集合的不相似度就与使用*k*-mer统计向量的不相似度几乎等价，因为后者对固定大小的输入，在$k$足够大时，其大多数元素都会是0和1。在这一意义下，基于MinHash的算法可以视作基于*k*-mer统计的算法在$k$较大情况下推广的近似。

Sourmash默认使用$k=31$。相比于TNF中的$k=4$，基于MinHash的算法可以更好地保留输入序列的模式信息。

#### Kaiju

Kaiju是一个对序列做分类学（taxonomic）分类（classify）工具，可以快速地预测读与contig的分类单元（taxon）。Kaiju将输入序列转换为蛋白质序列后再查找，由于蛋白质序列的保守性（conservativeness）和简并性（degeneracy）蛋白质序列查找相比于DNA序列查找更加健壮。

Kaiju利用Burrows–Wheeler变换（BWT）压缩数据库与快速查找序列。BWT算法也常用于序列对齐工具，例如Bowtie和BWA，该算法有着较好的查找效率和较小的内存占用。{{Menzel 2015}}

本文利用Kaiju作为单细胞测序数据的分类工具。

#### 其他序列分类工具

我们也使用到了rMLST的线上API对基因组组装草图（draft genome assembly）做分类学分类。rMLST利用核糖体多位点序列（ribosomal multilocus sequence）分类，能给出分类单元预测和对应分数。{{Jolley 2012}}

CAT（contig annotation tool）是对长DNA序列的分类学分类工具，利用Prodigal计算ORF并在NCBI的nr蛋白质数据库中查找，最终给出分类单元的最近公共祖先和对应分数。{{Cambuy 2016, Buchfink 2015, Hyatt 2010}}

#### DAS Tool

DAS Tool是结合多个装箱（bin）方法结果的一套方法流程。它用去重（dereplication）、聚合（aggregation）和打分（scoring）的策略去优化装箱结果，可以用于宏基因组学研究。DAS Tol可以提升组装质量，降低装箱冗余。{{Sieber 2018}} 

### 质量评估

质量评估与控制在数据处理流程中最晚，但也相当重要。基因组箱（genome bin）的质量指标包括完整性和污染度（contamination，与期望基因组大小相比的冗余率），contig长度的平均、中值与总和，以及N50统计（覆盖基因组全长50%的最长contig中的最短的长度）等。

CheckM是检验完整性和污染度的前沿工具，其原理基于估算标志基因（marker gene）与推断基因组的谱系（lineage）。CheckM是我们使用到的主要质量评估与控制工具。{{Parks 2015}}

质量评估与控制的结果也需要可视化的技巧来让读者更加容易理解，并迅速地抓住数据的关键特征。